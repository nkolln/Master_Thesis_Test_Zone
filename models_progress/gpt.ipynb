{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkolln/mt/vis10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import math\n",
    "import copy\n",
    "\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(dim,max_len=1000):\n",
    "    arr_pe = torch.zeros(max_len,dim)\n",
    "    #Creates an array for the positions of rows\n",
    "    arr_pos = torch.arange(0,max_len).unsqueeze(1)\n",
    "    #Sine and cosine are applied an alternating dims so half are generated(Sin and cos apply to same subset basically to have the different outputs)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.0) / dim))\n",
    "    #basically multipleis the 0-max len values by the div term which is unique per dim\n",
    "    temp = arr_pos*div_term\n",
    "    arr_pe[:,0::2]= torch.sin(temp)\n",
    "    arr_pe[:,1::2]= torch.cos(temp)\n",
    "    return(arr_pe.unsqueeze(0))\n",
    "\n",
    "def scaled_dot_product_attention(q,k,v,mask=None,dropout=None):\n",
    "    dim = q.size(-1)\n",
    "    q_k = torch.matmul(q,k.transpose(-2, -1))\n",
    "    q_k_scale = q_k * math.sqrt(dim)\n",
    "    #can add a mask here\n",
    "    if mask is not None:\n",
    "        q_k_scale = q_k_scale.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    q_k_scale = f.softmax(q_k_scale,dim=1)\n",
    "    if dropout is not None:\n",
    "        q_k_scale = dropout(q_k_scale)\n",
    "    attn = torch.matmul(q_k_scale,v)\n",
    "    return(attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0\n",
    "\n",
    "def run_tests():\n",
    "    for _ in range(10):\n",
    "        inference_test()\n",
    "def clones(component,N):\n",
    "    return(nn.ModuleList([copy.deepcopy(component)for _ in range(0,N)]))\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,h,dim,dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert dim % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = dim // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(dim, dim), 4)\n",
    "        self.attn = None\n",
    "        self.l_q = nn.Linear(dim,dim)\n",
    "        self.l_k = nn.Linear(dim,dim)\n",
    "        self.l_v = nn.Linear(dim,dim)\n",
    "        self.sdpa = scaled_dot_product_attention\n",
    "        self.dropout = nn.Dropout(p =dropout)\n",
    "\n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = q.size(0)\n",
    "        # q = self.l_q(q)\n",
    "        # k = self.l_k(k)\n",
    "        # v = self.l_v(v)\n",
    "        print(q.shape)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k)#.transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (q, k, v))\n",
    "        ]\n",
    "        print(query.shape,key.shape,value.shape)\n",
    "\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (q, k, v))\n",
    "        ]\n",
    "        print(query.shape,key.shape,value.shape)\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x = self.sdpa(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "Consists only of the transformer block of the original transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embed, positional encoding, decoder n layers, Linear, softmax\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self,decoder,embedding,N,dim):\n",
    "        super().__init__()\n",
    "        self.decoders = clones(decoder,N)\n",
    "        self.embed = embedding\n",
    "        #self.pe = positional_encoding\n",
    "        self.out_layer = nn.Linear(dim,dim)\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        x = self.embed(x)# + self.pe\n",
    "        for decode in self.decoders:\n",
    "            x = decode(x,mask)\n",
    "        x = self.out_layer(x)\n",
    "        x = f.softmax(x)\n",
    "        return(x)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512])\n",
      "torch.Size([1, 10, 8, 64]) torch.Size([1, 10, 8, 64]) torch.Size([1, 10, 8, 64])\n",
      "torch.Size([1, 8, 10, 64]) torch.Size([1, 8, 10, 64]) torch.Size([1, 8, 10, 64])\n",
      "torch.Size([1, 10, 512])\n",
      "torch.Size([1, 10, 8, 64]) torch.Size([1, 10, 8, 64]) torch.Size([1, 10, 8, 64])\n",
      "torch.Size([1, 8, 10, 64]) torch.Size([1, 8, 10, 64]) torch.Size([1, 8, 10, 64])\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_525/735758739.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = f.softmax(x)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPT' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 49\u001b[0m\n\u001b[1;32m     40\u001b[0m         ys \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m     41\u001b[0m             [ys, torch\u001b[39m.\u001b[39mempty(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtype_as(src\u001b[39m.\u001b[39mdata)\u001b[39m.\u001b[39mfill_(next_word)], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     44\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mExample Untrained Model Prediction:\u001b[39m\u001b[39m\"\u001b[39m, ys)\n\u001b[0;32m---> 49\u001b[0m show_example(run_tests)\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mshow_example\u001b[0;34m(fn, args)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_example\u001b[39m(fn, args\u001b[39m=\u001b[39m[]):\n\u001b[1;32m      2\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m RUN_EXAMPLES:\n\u001b[0;32m----> 3\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs)\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mrun_tests\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_tests\u001b[39m():\n\u001b[1;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m         inference_test()\n",
      "Cell \u001b[0;32mIn[14], line 34\u001b[0m, in \u001b[0;36minference_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(memory)\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m9\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     out \u001b[39m=\u001b[39m test_model\u001b[39m.\u001b[39;49mdecode(\n\u001b[1;32m     35\u001b[0m         memory, src_mask, ys, subsequent_mask(ys\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mtype_as(src\u001b[39m.\u001b[39mdata)\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     prob \u001b[39m=\u001b[39m test_model\u001b[39m.\u001b[39mgenerator(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     38\u001b[0m     _, next_word \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(prob, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/mt/vis10/lib/python3.10/site-packages/torch/nn/modules/module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1264\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1265\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1266\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "def make_model(\n",
    "    src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = GPT(\n",
    "        DecoderLayer(d_model, c(attn), c(ff), dropout),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        N,\n",
    "        d_model,\n",
    "    )\n",
    "\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "def inference_test():\n",
    "    test_model = make_model(11, 11, 2)\n",
    "    test_model.eval()\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "    memory = test_model.forward(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).type_as(src)\n",
    "    print(memory)\n",
    "\n",
    "    for i in range(9):\n",
    "        out = test_model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "\n",
    "    print(\"Example Untrained Model Prediction:\", ys)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "show_example(run_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GPTTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout=0.1):\n",
    "        super(GPTTransformer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.pos_encoding = PositionalEncoding(hidden_size, dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([GPTTransformerLayer(hidden_size, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, input_ids, input_mask=None):\n",
    "        # Perform embedding and add positional encoding\n",
    "        embedded = self.embedding(input_ids)\n",
    "        embedded = self.pos_encoding(embedded)\n",
    "\n",
    "        # Pass the input through the layers of the transformer\n",
    "        for layer in self.layers:\n",
    "            embedded = layer(embedded, input_mask)\n",
    "\n",
    "        # Apply the output layer and return the result\n",
    "        logits = self.output_layer(embedded)\n",
    "        return logits\n",
    "\n",
    "class GPTTransformerLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout=0.1):\n",
    "        super(GPTTransformerLayer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = MultiHeadAttention(hidden_size, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * hidden_size, hidden_size)\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, input_mask=None):\n",
    "        # Apply attention\n",
    "        attention_output = self.attention(input, input, input, input_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        attention_output = attention_output + input\n",
    "\n",
    "        # Apply feed-forward layer\n",
    "        feed_forward_output = self.feed_forward(attention_output)\n",
    "        feed_forward_output = self.dropout2(feed_forward_output)\n",
    "        feed_forward_output = feed_forward_output + attention_output\n",
    "\n",
    "        return feed_forward_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, num_heads, dropout):\n",
    "        super(GPT, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(hidden_size, num_heads, num_layers, dropout)\n",
    "        self.output_linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x, mask)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n",
    "\n",
    "# Create a GPT model with a vocabulary size of 50, hidden size of 32,\n",
    "# 2 layers, 4 heads, and a dropout rate of 0.1\n",
    "gpt = GPT(50, 32, 2, 4, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set the device to run on: GPU or CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set the number of epochs and the learning rate\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Set the criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(gpt.parameters(), lr=learning_rate)\n",
    "\n",
    "# Move the model to the specified device\n",
    "gpt.to(device)\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    gpt.train()\n",
    "\n",
    "    # Loop over the training data\n",
    "    for x, y in train_data:\n",
    "        # Move the input and label tensors to the correct device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = gpt(x, mask)\n",
    "        loss = criterion(output.view(-1, gpt.vocab_size), y.view(-1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss for each epoch\n",
    "    print('Epoch: {}, Loss: {:.4f}'.format(epoch+1, loss.item()))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "gpt.eval()\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in test_data:\n",
    "        # Move the input and label tensors to the correct device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = gpt(x, mask)\n",
    "\n",
    "        # Get the predictions\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        # Update the number of correct and total predictions\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "    # Print the accuracy\n",
    "    print('Accuracy: {:.2f}%'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_tokens, embedding_dim, num_heads, hidden_dim, num_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.num_tokens = num_tokens\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(num_tokens, embedding_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = nn.Embedding(num_tokens, embedding_dim)\n",
    "        \n",
    "        # Multi-head attention layers\n",
    "        self.attention_layers = nn.ModuleList([nn.MultiheadAttention(embedding_dim, num_heads) for _ in range(num_layers)])\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.ModuleList([nn.Linear(embedding_dim, hidden_dim), nn.Linear(hidden_dim, num_tokens)])\n",
    "        \n",
    "    def forward(self, input_tokens, input_lengths):\n",
    "        # Add positional encoding\n",
    "        positions = torch.arange(input_tokens.size(1), device=input_tokens.device).unsqueeze(0).repeat(input_tokens.size(0), 1)\n",
    "        input_tokens = input_tokens + self.positional_encoding(positions)\n",
    "        \n",
    "        # Embed input tokens\n",
    "        embedded = self.embedding(input_tokens)\n",
    "        \n",
    "        # Pass through multi-head attention layers\n",
    "        for attention_layer in self.attention_layers:\n",
    "            embedded = attention_layer(embedded, embedded, embedded, input_lengths)\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        hidden = self.fc_layers[0](embedded)\n",
    "        output = self.fc_layers[1](hidden)\n",
    "        \n",
    "        return output\n",
    "\n",
    "model = Transformer(num_tokens=10000, embedding_dim=512, num_heads=8, hidden_dim=1024, num_layers=6)\n",
    "output = model(input_tokens, input_lengths)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vis10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5328d19f01c96b26b2b58c1c0757ffb719f3cd8060448ceb902d5879c439f16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
